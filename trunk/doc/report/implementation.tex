\section{Implementation} 
\paragraph
At this section we describe the steps we followed to achieve our experiments. First we figured out our crawling procedure and we decided to use libcurl [x] for our HTTP requests and a list of the top sites from Alexa [x] to feed the crawler. We use the "follow locations" option, since some sites send a Location header to another URL and we need just the last one. The website checking procedure consists of two main types: Header processing and Body data processing. Firstly, we check if the returned headers match with some frequently used headers for CSRF defense, using the lib pcre regular expression pattern macher [x]. After the header processing, and if the returned status code says that everything was fine (200 OK), we start observing the html code via the libxml library [x]. We analyse the tags and search for hidden input fields and if they match our patterns we declare them as secret validation tokens used for CSRF defense. In order to learn more about this website, we follow all the links we find (??) in depth of one and do the same. (Lookup for Register links ??) + Referer checking...
\paragraph
